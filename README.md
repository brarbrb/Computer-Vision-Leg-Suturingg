# Computer-Vision-Leg-Suturingg
This repository provides a pipeline for training and evaluating YOLO models to detect surgical tools (e.g., tweezers, needle driver) in images and videos. It includes data augmentation configurations, model comparisons, prediction scripts, and exploratory data analysis (EDA).

## 📁 Project Structure
├── video.py # Predicts bounding boxes on a video and saves the annotated output

├── predict.py # Predicts bounding boxes on a single image and visualizes the output

├── comparing_models.ipynb # Notebook comparing performance of different models

├── EDA_training_models.ipynb # Exploratory Data Analysis and training steps

├── surgical.yaml # Main training configuration for surgical dataset for base model with no modifications

├── copied_data.yaml # Configuration for Yolo models using original data + data with custom augmentation, pooled frames from in-distribution and out of distribution videos

## Model weights
[Download final model weights here](https://github.com/brarbrb/Computer-Vision-Leg-Suturingg/releases/download/v1/fine_tuned_pseudo_ood.pt)

## 🛠️ Requirements

- Python 3.8+
- [Ultralytics YOLOv9](https://docs.ultralytics.com/)
- OpenCV
- Matplotlib

For efficient training use GPU: We had Tesla T100

Install dependencies:
```bash
pip install -r requirements.txt
```

Models
The YOLOv9 model is used for surgical tool detection in leg suturing surgery with the following classes:
- 0: Empty (Red Color of the boxes)

- 1: Tweezers (Green Color of the boxes)

- 2: Needle_driver (Blue Color of the boxes)


📊 **Notebooks**

`comparing_models.ipynb` — Compare loss and mAp of different models.

`EDA_training_models.ipynb` — Analyzing the dataset, training the models, pipeline for custom augmentation and pooling videos from ID and OOD videos. 


⚙️ **YOLO Config Files**

YOLO model uses configuration yaml files for training. The usual structure is 
```
dataset/
├── images/
│   ├── train/
│   │   ├── img1.jpg
│   │   ├── img2.jpg
│   ├── val/
│   │   ├── img3.jpg
│   │   ├── img4.jpg
│
├── labels/
│   ├── train/
│   │   ├── img1.txt
│   │   ├── img2.txt
│   ├── val/
│   │   ├── img3.txt
│   │   ├── img4.txt
```
Each line in the .txt file represents a box and must follow this format:
`class x_center y_center box_width box_height`

So we need to define configuration files that specify the paths to the dataset:

`surgical.yaml`: Defines training/validation data paths and class labels for YOLOv9 models

`copied_data.yaml`: Defines data path for semi-supervised training (first for In Distribution videos and in the next step for out of distribution video)

*Note*: In distribution means videos from the same settings (surgery, video, room etc) as the training dataset was from out of distribution (videos that have slightly different settings) 

## 🎥 Run Inference on Videos
```bash 
python video.py path/to/input.mp4 path/to/output.mp4 --model path/to/your_model.pt
```
Default model path: `runs/detect/base_model2/weights/best.pt`


## 🖼️ Run Inference on Images
```bash
python predict.py path/to/input.jpg path/to/output.jpg --model path/to/your_model.pt
```
Default model path: `trained_models/base_model2.pt`

## Files and Directories that created from running the `EDA_training_models.ipynb`

1. Model Weights Repositories:
`runs/detect/base_model2/...`	YOLO training output folder (autogenerated)
`trained_models/base_model2.pt`	Manually managed trained model directory
2. `runs` repository is autogenerated by YOLO model to store weights (last and best) 

3. `data` Image Directory for Training: to store extracted video frames that were used in semi-supervised learning in the needed structure for fine-tuning YOLO model
