# Computer-Vision-Leg-Suturingg
This repository provides a pipeline for training and evaluating YOLO models to detect surgical tools (e.g., tweezers, needle driver) in images and videos. It includes data augmentation configurations, model comparisons, prediction scripts, and exploratory data analysis (EDA).

## 📁 Project Structure
├── video.py # Predicts bounding boxes on a video and saves the annotated output

├── predict.py # Predicts bounding boxes on a single image and visualizes the output

├── comparing_models.ipynb # Notebook comparing performance of different models

├── EDA_training_models.ipynb # Exploratory Data Analysis and training steps

├── custom_aug.yaml # Custom data augmentation configuration for yolov9

├── pseudo.yaml # Pseudo-labeling dataset config for yolov9 (on in distribution videos)

├── pseudo_ood.yaml # Pseudo-labeling for out-of-distribution data

├── surgical.yaml # Main training configuration for surgical dataset for base model with no modifications


## 🛠️ Requirements

- Python 3.8+
- [Ultralytics YOLOv9](https://docs.ultralytics.com/)
- OpenCV
- Matplotlib

For efficient training use GPU: We had Tesla T100

Install dependencies:
```bash
pip install -r requirements.txt
```

Models
The YOLOv9 model is used for surgical tool detection in leg suturing surgery with the following classes:
- 0: Empty (Red Color of the boxes)

- 1: Tweezers (Green Color of the boxes)

- 2: Needle_driver (Blue Color of the boxes)


📊 **Notebooks**

`comparing_models.ipynb` — Compare loss and mAp of different models.

`EDA_training_models.ipynb` — Analyzing the dataset, training the models, pipeline for custom augmentation and pooling videos from ID and OOD videos. 


⚙️ **YOLO Config Files**

YOLO model uses configuration yaml files for training. The usual structure is 
```
dataset/
├── images/
│   ├── train/
│   │   ├── img1.jpg
│   │   ├── img2.jpg
│   ├── val/
│   │   ├── img3.jpg
│   │   ├── img4.jpg
│
├── labels/
│   ├── train/
│   │   ├── img1.txt
│   │   ├── img2.txt
│   ├── val/
│   │   ├── img3.txt
│   │   ├── img4.txt
```
Each line, represents box and in the .txt label file must follow this format:
`class x_center y_center box_width box_height`

So we need to define configuration files that specify the paths to the dataset:

`surgical.yaml`: Defines training/validation data paths and class labels for YOLOv9 models

`pseudo.yaml`, `pseudo_ood.yaml`: Defines data path for semi-supervised training (first for In Distribution videos and in the next step for out of distribution video)

`custom_aug.yaml`: Custom augmentation strategies create new files, so we needed to change the configurations 

*Note*: In distribution means videos from the same settings (surgery, video, room etc) as the training dataset was from out of distribution (videos that have slightly different settings) 

## 🎥 Run Inference on Videos
```bash 
python video.py path/to/input.mp4 path/to/output.mp4 --model path/to/your_model.pt
```
Default model path: `runs/detect/base_model2/weights/best.pt`


## 🖼️ Run Inference on Images
```bash
python predict.py path/to/input.jpg path/to/output.jpg --model path/to/your_model.pt
```
Default model path: `trained_models/base_model2.pt`

## Files and Directories that created from running the `EDA_training_models.ipynb`

1. Model Weights Repositories:
`runs/detect/base_model2/...`	YOLO training output folder (autogenerated)
`trained_models/base_model2.pt`	Manually managed trained model directory

2. Image Directory Creation for Training: to store extracted video frames or images, commonly used in semi-supervised learning:

Typical name: image_dir, likely something like "data/images" or "pseudo_labels/images" depending on your variable.

Purpose: Save images (e.g. extracted video frames) to be used for pseudo-labeling or training.
